# Real-time Translator Configuration
# Edit this file to customize your settings

[api]
# OpenAI-compatible API settings
# For OpenAI: Leave base_url empty or set to https://api.openai.com/v1
# For SiliconFlow: https://api.siliconflow.cn/v1
# For Ollama: http://localhost:11434/v1
#base_url = https://huzi-baozi.com/k2-thinking-api/v1
base_url = http://localhost:11434/v1/
api_key = sk-talkai-bd76c0f60c6f4e50bcfa39fc1dce0efd

[translation]
# Model to use for translation
# model = MBZUAI-IFM/K2-Think-nothink
model = qwen2.5:7b
# Target language for translation
target_lang = Chinese

[display]
# Minimum seconds to show each translation on screen
display_duration = 10
# Window size
window_width = 800
window_height = 120

[transcription]
# Whisper model size: tiny, base, small, medium, large-v2
whisper_model = base
# Device: cpu, cuda, or auto (auto will use Metal on Apple Silicon)
device = auto
# Compute type: int8, float16, int8_float16, float32
# On Apple Silicon, try float16 for better speed
compute_type = float16
# Source language: auto, en, zh, ja, ko, es, fr, de, etc.
source_language = auto
# Number of concurrent Whisper workers (more = faster but more RAM, ~150MB each)
transcription_workers = 4

[audio]
# Device index for audio input.
# 'auto' = auto-detect BlackHole device (recommended for system audio)
# Empty or number = use specific device index
device_index = auto

# Audio sample rate (16000 recommended for Whisper)
sample_rate = 16000
# Silence threshold for VAD (0.01 = sensitive, 0.05 = less sensitive)
silence_threshold = 0.005
# Seconds of silence before cutting a phrase
silence_duration = 0.5
# Audio chunk duration in seconds
chunk_duration = 0.1
# Maximum phrase duration - force processing after this many seconds even if no silence
max_phrase_duration = 3

# Streaming mode (recommended for real-time)
# When true: emits audio at fixed intervals instead of waiting for silence
streaming_mode = true
# Seconds between audio emissions in streaming mode
streaming_interval = 1.5
# Seconds of overlap between chunks for context continuity
streaming_overlap = 0.3
